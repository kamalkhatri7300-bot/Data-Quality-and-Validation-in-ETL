{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : Define Data Quality in the context of ETL pipelines. Why is it more than just data cleaning?\n",
        "\n",
        "- Answer 1\n",
        "\n",
        "Data Quality is the measure of how well a dataset serves its intended purpose in decision-making and operations. It is evaluated based on dimensions like accuracy, completeness, consistency, timeliness, and validity.\n",
        "\n",
        "\n",
        "Question 2 : Explain why poor data quality leads to misleading dashboards and incorrect decisions.\n",
        "\n",
        "- Answer 2\n",
        "\n",
        "Dashboards rely on the \"Garbage In, Garbage Out\" (GIGO) principle. Poor data quality leads to:\n",
        "\n",
        "Inaccurate Metrics: If Txn_Amount has nulls or zeros, your \"Total Revenue\" will be understated.\n",
        "\n",
        "Skewed Trends: Duplicate records can inflate performance metrics, making a business look more successful than it actually is.\n",
        "\n",
        "Lack of Trust: When stakeholders spot obvious errors, they stop relying on the dashboard for strategic decisions.\n",
        "\n",
        "Question 3 : What is duplicate data? Explain three causes in ETL pipelines.\n",
        "\n",
        "- Answer 3\n",
        "\n",
        "Duplicate data occurs when multiple records represent the same real-world entity or event.\n",
        "Three causes in ETL:\n",
        "\n",
        "Source System Issues: The same transaction is entered twice in the source database.\n",
        "\n",
        "Retry Logic Failures: An ETL job fails halfway and restarts without \"idempotency,\" loading the same batch of data again.\n",
        "\n",
        "Incorrect Joins: Joining two tables on a non-unique key can cause records to multiply.\n",
        "\n",
        "Question 4 : Differentiate between exact, partial, and fuzzy duplicates.\n",
        "\n",
        "- Answer 4\n",
        "\n",
        "Exact,\n",
        "\n",
        "Every single column in the records matches perfectly.\n",
        "\n",
        "Partial,\n",
        "\n",
        "\"Key identifiers match, but some non-essential columns differ.\"\n",
        "\n",
        "Fuzzy,\n",
        "\n",
        "Records are similar but not identical due to typos or formatting.\n",
        "\n",
        "\n",
        "Question 5 : Why should data validation be performed during transformation rather than after loading?\n",
        "\n",
        "- Answer 5\n",
        "\n",
        "Validating during Transformation (the \"T\" in ETL) follows the \"Shift Left\" principle:\n",
        "\n",
        "Prevents Corruption: It keeps \"dirty data\" out of the Data Warehouse.\n",
        "\n",
        "Efficiency: It is easier to reject or fix a record before it is committed to a permanent table.\n",
        "\n",
        "Lineage: You can log errors immediately relative to the source, making it easier to debug the root cause.\n",
        "\n",
        "\n",
        "Question 6 : Explain how business rules help in validating data accuracy. Give an example.\n",
        "\n",
        "\n",
        "Use the following Sales_Transactions dataset for all questions:\n",
        "\n",
        "- Answeer 6\n",
        "\n",
        "Business rules are logic-based constraints that reflect real-world company policies. They ensure data is not just \"formatted correctly\" but makes \"logical sense.\"\n",
        "\n",
        "Example: A business rule might state that \"Quantity cannot be less than 1.\" In your dataset, record 205 has a Null quantity; a validation rule would flag this as a violation of business logic.\n",
        "\n",
        "\n",
        "Question 7 : Write an SQL query on Sales_Transactions to list all duplicate keys and their counts using the business key (Customer_ID + Product_ID + Txn_Date + Txn_Amount )\n",
        "\n",
        "- Answer 7\n",
        "\n",
        "SELECT\n",
        "    Customer_ID,\n",
        "    Product_ID,\n",
        "    Txn_Date,\n",
        "    Txn_Amount,\n",
        "    COUNT(*) as Duplicate_Count\n",
        "FROM Sales_Transactions\n",
        "GROUP BY Customer_ID, Product_ID, Txn_Date, Txn_Amount\n",
        "HAVING COUNT(*) > 1;\n",
        "\n",
        "\n",
        "Question 8 : Enforcing Referential Integrity\n",
        "\n",
        "Assume the following table:\n",
        "Identify Sales_Transactions.Customer_ID values that violate referential integrity when joined with Customers_Master and write a query to detect such violations.\n",
        "\n",
        "- Answer 8\n",
        "\n",
        "SELECT ST.*\n",
        "FROM Sales_Transactions ST\n",
        "LEFT JOIN Customers_Master CM ON ST.Customer_ID = CM.CustomerID\n",
        "WHERE CM.CustomerID IS NULL;"
      ],
      "metadata": {
        "id": "1N-IbHAb6M79"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CVISv2e59gk"
      },
      "outputs": [],
      "source": []
    }
  ]
}